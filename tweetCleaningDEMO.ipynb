{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#df.dropna(subset=['text'], inplace=True)  # Drop rows where tweets are missing\n",
    "#df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def clean_tweet_spacy(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions (@user)\n",
    "    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "\n",
    "    doc = nlp(text)  # Process text using spaCy\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop]  # Lemmatization & stopword removal\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df['cleaned_tweet'] = df['text'].apply(clean_tweet_spacy)\n",
    "print(df[['text', 'cleaned_tweet']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(df['cleaned_tweet'].dtype)  # Should be \"object\" (string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(df.dtypes)  # Check data types of all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(df['cleaned_tweet'].head())  # Check first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(df['cleaned_tweet'].isnull().sum())  # Count missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df['cleaned_tweet'] = df['cleaned_tweet'].fillna(\"\")\n",
    "df['cleaned_tweet'] = df['cleaned_tweet'].astype(str)\n",
    "\n",
    "df['cleaned_tweet'] = df['cleaned_tweet'].astype(str).fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "all_words = ' '.join(df['cleaned_tweet'].dropna())  # Drop NaN values before joining\n",
    "# Join all tweets into a single string\n",
    "all_words = ' '.join(df['cleaned_tweet'])\n",
    "print(type(all_words))  # Should be <class 'str'>\n",
    "print(all_words[:500])  # Print first 500 characters to check for issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Generate the WordCloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', collocations=False).generate(all_words)\n",
    "\n",
    "if not all_words.strip():  # Check if string is empty\n",
    "    print(\"Error: No valid text found for WordCloud\")\n",
    "\n",
    "# Plot the WordCloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_list = all_words.split()\n",
    "word_counts = Counter(word_list)\n",
    "print(word_counts.most_common(10))  # Top 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df['tweet_length'] = df['cleaned_tweet'].apply(lambda x: len(x.split()))\n",
    "\n",
    "sns.histplot(df['tweet_length'], bins=30, kde=True)\n",
    "plt.xlabel('Tweet Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Tweet Lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def get_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity  # Score between -1 (negative) to 1 (positive)\n",
    "\n",
    "df['sentiment'] = df['cleaned_tweet'].apply(get_sentiment)\n",
    "\n",
    "sns.histplot(df['sentiment'], bins=30, kde=True)\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Tweet Sentiment Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
